"""
è©•ä¼° DLRM æ¨¡å‹çš„æ¨è–¦æ•ˆæœ
è¨ˆç®— NDCG@k, Recall@k, Precision@k, MRR, MAP ç­‰æŒ‡æ¨™
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
from pathlib import Path
from collections import defaultdict

from dlrm_model import create_travel_dlrm
from route_aware_recommender import create_route_aware_recommender


class RecommendationMetrics:
    """æ¨è–¦ç³»çµ±è©•ä¼°æŒ‡æ¨™"""
    
    @staticmethod
    def ndcg_at_k(predictions: List[int], ground_truth: List[int], k: int) -> float:
        """
        è¨ˆç®— NDCG@k (Normalized Discounted Cumulative Gain)
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨ï¼ˆæŒ‰æ¨è–¦é †åºï¼‰
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
            k: æˆªæ–·ä½ç½®
        
        Returns:
            NDCG@k åˆ†æ•¸ [0, 1]
        """
        if not ground_truth:
            return 0.0
        
        # DCG@k
        dcg = 0.0
        for i, pred_id in enumerate(predictions[:k]):
            if pred_id in ground_truth:
                # rel = 1 if relevant, 0 otherwise
                dcg += 1.0 / np.log2(i + 2)  # i+2 because index starts at 0
        
        # IDCG@k (ç†æƒ³æƒ…æ³ï¼šå‰kå€‹éƒ½æ˜¯ç›¸é—œçš„)
        idcg = 0.0
        for i in range(min(k, len(ground_truth))):
            idcg += 1.0 / np.log2(i + 2)
        
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    @staticmethod
    def recall_at_k(predictions: List[int], ground_truth: List[int], k: int) -> float:
        """
        è¨ˆç®— Recall@k
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
            k: æˆªæ–·ä½ç½®
        
        Returns:
            Recall@k åˆ†æ•¸ [0, 1]
        """
        if not ground_truth:
            return 0.0
        
        top_k_preds = set(predictions[:k])
        relevant = set(ground_truth)
        
        hits = len(top_k_preds & relevant)
        return hits / len(relevant)
    
    @staticmethod
    def precision_at_k(predictions: List[int], ground_truth: List[int], k: int) -> float:
        """
        è¨ˆç®— Precision@k
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
            k: æˆªæ–·ä½ç½®
        
        Returns:
            Precision@k åˆ†æ•¸ [0, 1]
        """
        if k == 0:
            return 0.0
        
        top_k_preds = set(predictions[:k])
        relevant = set(ground_truth)
        
        hits = len(top_k_preds & relevant)
        return hits / k
    
    @staticmethod
    def hit_rate_at_k(predictions: List[int], ground_truth: List[int], k: int) -> float:
        """
        è¨ˆç®— Hit Rate@k (è‡³å°‘å‘½ä¸­ä¸€å€‹)
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
            k: æˆªæ–·ä½ç½®
        
        Returns:
            Hit Rate@k: 1.0 if hit, 0.0 otherwise
        """
        top_k_preds = set(predictions[:k])
        relevant = set(ground_truth)
        
        return 1.0 if len(top_k_preds & relevant) > 0 else 0.0
    
    @staticmethod
    def mrr(predictions: List[int], ground_truth: List[int]) -> float:
        """
        è¨ˆç®— MRR (Mean Reciprocal Rank)
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
        
        Returns:
            MRR åˆ†æ•¸
        """
        relevant = set(ground_truth)
        
        for i, pred_id in enumerate(predictions):
            if pred_id in relevant:
                return 1.0 / (i + 1)
        
        return 0.0
    
    @staticmethod
    def average_precision(predictions: List[int], ground_truth: List[int]) -> float:
        """
        è¨ˆç®— Average Precision (AP)
        
        Args:
            predictions: é æ¸¬çš„ POI ID åˆ—è¡¨
            ground_truth: çœŸå¯¦äº’å‹•çš„ POI ID åˆ—è¡¨
        
        Returns:
            AP åˆ†æ•¸
        """
        if not ground_truth:
            return 0.0
        
        relevant = set(ground_truth)
        hits = 0
        sum_precisions = 0.0
        
        for i, pred_id in enumerate(predictions):
            if pred_id in relevant:
                hits += 1
                precision_at_i = hits / (i + 1)
                sum_precisions += precision_at_i
        
        if hits == 0:
            return 0.0
        
        return sum_precisions / len(relevant)


class ModelEvaluator:
    """æ¨¡å‹è©•ä¼°å™¨"""
    
    def __init__(
        self,
        model_path: str = "models/travel_dlrm.pth",
        device: str = "cpu"
    ):
        self.model_path = model_path
        self.device = torch.device(device)
        self.model = None
        self.recommender = None
        self.metrics = RecommendationMetrics()
        
    def load_model(self):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {self.model_path}")
        
        try:
            # è¼‰å…¥ checkpoint
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            # å¾ checkpoint ç²å–æ¨¡å‹é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'config' in checkpoint:
                config = checkpoint['config']
                print(f"   é…ç½®: {config}")
            else:
                # ä½¿ç”¨é è¨­é…ç½®
                config = {
                    'user_continuous_dim': 10,
                    'poi_continuous_dim': 8,
                    'path_continuous_dim': 4,
                    'poi_vocab_sizes': {
                        'category': 100,
                        'state': 50,
                        'price_level': 5
                    },
                    'user_vocab_sizes': {},
                    'embedding_dim': 64
                }
                print(f"   ä½¿ç”¨é è¨­é…ç½®")
            
            # å‰µå»ºæ¨¡å‹
            self.model = create_travel_dlrm(**config)
            
            # è¼‰å…¥æ¬Šé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ“ æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")
            else:
                self.model.load_state_dict(checkpoint)
                print(f"âœ“ æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")
            
            self.model.to(self.device)
            self.model.eval()
            
            # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
            num_params = sum(p.numel() for p in self.model.parameters())
            print(f"   åƒæ•¸é‡: {num_params:,}")
            
            if 'epoch' in checkpoint:
                print(f"   è¨“ç·´è¼ªæ¬¡: {checkpoint['epoch']}")
            if 'best_loss' in checkpoint:
                print(f"   æœ€ä½³æå¤±: {checkpoint['best_loss']:.4f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def load_test_data(self, test_data_path: Optional[str] = None) -> List[Dict]:
        """
        è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        
        Args:
            test_data_path: æ¸¬è©¦æ•¸æ“šè·¯å¾‘ï¼ˆJSONæ ¼å¼ï¼‰
        
        Returns:
            æ¸¬è©¦æ¨£æœ¬åˆ—è¡¨
        """
        if test_data_path and Path(test_data_path).exists():
            print(f"ğŸ“‚ è¼‰å…¥æ¸¬è©¦æ•¸æ“š: {test_data_path}")
            with open(test_data_path, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            print(f"âœ“ è¼‰å…¥ {len(test_data)} å€‹æ¸¬è©¦æ¨£æœ¬")
            return test_data
        else:
            print("âš ï¸ æœªæä¾›æ¸¬è©¦æ•¸æ“šï¼Œç”Ÿæˆæ¨¡æ“¬æ•¸æ“š...")
            return self._generate_mock_test_data()
    
    def _generate_mock_test_data(self, num_users: int = 50) -> List[Dict]:
        """ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦æ•¸æ“š"""
        test_data = []
        
        # æ¨¡æ“¬ç”¨æˆ¶å’ŒPOI
        for user_id in range(num_users):
            sample = {
                'user_id': f"user_{user_id}",
                'user_history': [
                    {'poi_id': f"poi_{i}", 'rating': 4.0 + np.random.rand()}
                    for i in range(5)
                ],
                'start_location': (37.7749 + np.random.randn() * 0.01, 
                                  -122.4194 + np.random.randn() * 0.01),
                'end_location': (37.7849 + np.random.randn() * 0.01, 
                                -122.4094 + np.random.randn() * 0.01),
                'ground_truth': [f"poi_{i}" for i in range(100, 105)],  # çœŸå¯¦äº’å‹•çš„POI
                'activity_intent': 'æ—…éŠæ¢ç´¢'
            }
            test_data.append(sample)
        
        print(f"âœ“ ç”Ÿæˆ {len(test_data)} å€‹æ¨¡æ“¬æ¸¬è©¦æ¨£æœ¬")
        return test_data
    
    def evaluate(
        self,
        test_data: List[Dict],
        k_values: List[int] = [5, 10, 20],
        top_k: int = 50
    ) -> Dict:
        """
        è©•ä¼°æ¨¡å‹
        
        Args:
            test_data: æ¸¬è©¦æ•¸æ“šåˆ—è¡¨
            k_values: è¦è¨ˆç®—çš„ k å€¼åˆ—è¡¨
            top_k: ç”Ÿæˆçš„æ¨è–¦æ•¸é‡
        
        Returns:
            è©•ä¼°çµæœå­—å…¸
        """
        print(f"\nğŸ¯ é–‹å§‹è©•ä¼°æ¨¡å‹...")
        print(f"   æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_data)}")
        print(f"   è©•ä¼°æŒ‡æ¨™: NDCG@k, Recall@k, Precision@k, Hit Rate@k, MRR, MAP")
        print(f"   k å€¼: {k_values}")
        
        # åˆå§‹åŒ–çµæœå®¹å™¨
        results = {
            'ndcg': {k: [] for k in k_values},
            'recall': {k: [] for k in k_values},
            'precision': {k: [] for k in k_values},
            'hit_rate': {k: [] for k in k_values},
            'mrr': [],
            'map': []
        }
        
        # è¼‰å…¥æ¨è–¦å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.recommender is None:
            print("\nğŸ“¦ åˆå§‹åŒ–æ¨è–¦å™¨...")
            try:
                self.recommender = create_route_aware_recommender(
                    model_checkpoint=self.model_path,
                    device=str(self.device)
                )
            except Exception as e:
                print(f"âš ï¸ æ¨è–¦å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
                print("âš ï¸ å°‡ä½¿ç”¨ç°¡åŒ–è©•ä¼°æ¨¡å¼ï¼ˆåƒ…æ¨¡å‹æ¨ç†ï¼‰")
        
        # å°æ¯å€‹æ¸¬è©¦æ¨£æœ¬é€²è¡Œè©•ä¼°
        for idx, sample in enumerate(test_data):
            if (idx + 1) % 10 == 0:
                print(f"   é€²åº¦: {idx + 1}/{len(test_data)}")
            
            try:
                # ç²å–æ¨è–¦çµæœ
                if self.recommender:
                    recommendations = self.recommender.recommend_on_route(
                        user_id=sample['user_id'],
                        user_history=sample.get('user_history', []),
                        start_location=sample['start_location'],
                        end_location=sample['end_location'],
                        activityIntent=sample.get('activity_intent', 'æ—…éŠæ¢ç´¢'),
                        top_k=top_k
                    )
                    
                    # æå–æ¨è–¦çš„ POI ID
                    predicted_ids = [rec['poi'].get('poi_id', rec['poi'].get('name', f"poi_{i}")) 
                                    for i, rec in enumerate(recommendations)]
                else:
                    # ç°¡åŒ–æ¨¡å¼ï¼šç”Ÿæˆéš¨æ©Ÿé æ¸¬
                    predicted_ids = [f"poi_{i}" for i in range(top_k)]
                
                ground_truth = sample.get('ground_truth', [])
                
                # è¨ˆç®—å„é …æŒ‡æ¨™
                for k in k_values:
                    results['ndcg'][k].append(
                        self.metrics.ndcg_at_k(predicted_ids, ground_truth, k)
                    )
                    results['recall'][k].append(
                        self.metrics.recall_at_k(predicted_ids, ground_truth, k)
                    )
                    results['precision'][k].append(
                        self.metrics.precision_at_k(predicted_ids, ground_truth, k)
                    )
                    results['hit_rate'][k].append(
                        self.metrics.hit_rate_at_k(predicted_ids, ground_truth, k)
                    )
                
                results['mrr'].append(
                    self.metrics.mrr(predicted_ids, ground_truth)
                )
                results['map'].append(
                    self.metrics.average_precision(predicted_ids, ground_truth)
                )
                
            except Exception as e:
                print(f"âš ï¸ æ¨£æœ¬ {idx} è©•ä¼°å¤±æ•—: {e}")
                continue
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_results = {
            'ndcg': {k: np.mean(v) if v else 0.0 for k, v in results['ndcg'].items()},
            'recall': {k: np.mean(v) if v else 0.0 for k, v in results['recall'].items()},
            'precision': {k: np.mean(v) if v else 0.0 for k, v in results['precision'].items()},
            'hit_rate': {k: np.mean(v) if v else 0.0 for k, v in results['hit_rate'].items()},
            'mrr': np.mean(results['mrr']) if results['mrr'] else 0.0,
            'map': np.mean(results['map']) if results['map'] else 0.0,
            'num_samples': len(test_data)
        }
        
        return avg_results
    
    def print_results(self, results: Dict):
        """æ‰“å°è©•ä¼°çµæœ"""
        print("\n" + "="*60)
        print("ğŸ“Š è©•ä¼°çµæœ")
        print("="*60)
        
        print(f"\næ¨£æœ¬æ•¸: {results['num_samples']}")
        
        print(f"\nğŸ“ˆ NDCG (Normalized Discounted Cumulative Gain):")
        for k, score in results['ndcg'].items():
            print(f"   NDCG@{k:2d}: {score:.4f}")
        
        print(f"\nğŸ“ˆ Recall:")
        for k, score in results['recall'].items():
            print(f"   Recall@{k:2d}: {score:.4f}")
        
        print(f"\nğŸ“ˆ Precision:")
        for k, score in results['precision'].items():
            print(f"   Precision@{k:2d}: {score:.4f}")
        
        print(f"\nğŸ“ˆ Hit Rate:")
        for k, score in results['hit_rate'].items():
            print(f"   Hit Rate@{k:2d}: {score:.4f}")
        
        print(f"\nğŸ“ˆ å…¶ä»–æŒ‡æ¨™:")
        print(f"   MRR (Mean Reciprocal Rank): {results['mrr']:.4f}")
        print(f"   MAP (Mean Average Precision): {results['map']:.4f}")
        
        print("\n" + "="*60)
    
    def save_results(self, results: Dict, output_path: str = "evaluation_results.json"):
        """ä¿å­˜è©•ä¼°çµæœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜è‡³: {output_path}")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è©•ä¼° Travel DLRM æ¨¡å‹')
    parser.add_argument('--model', type=str, default='models/travel_dlrm.pth',
                       help='æ¨¡å‹æ¬Šé‡è·¯å¾‘')
    parser.add_argument('--test-data', type=str, default=None,
                       help='æ¸¬è©¦æ•¸æ“šè·¯å¾‘ (JSON)')
    parser.add_argument('--device', type=str, default='cpu',
                       choices=['cpu', 'cuda', 'mps'],
                       help='é‹ç®—è¨­å‚™')
    parser.add_argument('--k-values', type=int, nargs='+', default=[5, 10, 20],
                       help='è©•ä¼°çš„ k å€¼åˆ—è¡¨')
    parser.add_argument('--top-k', type=int, default=50,
                       help='ç”Ÿæˆçš„æ¨è–¦æ•¸é‡')
    parser.add_argument('--output', type=str, default='evaluation_results.json',
                       help='çµæœè¼¸å‡ºè·¯å¾‘')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸ¯ Travel DLRM æ¨¡å‹è©•ä¼°")
    print("="*60)
    
    # å‰µå»ºè©•ä¼°å™¨
    evaluator = ModelEvaluator(
        model_path=args.model,
        device=args.device
    )
    
    # è¼‰å…¥æ¨¡å‹
    if not evaluator.load_model():
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œé€€å‡ºè©•ä¼°")
        return
    
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    test_data = evaluator.load_test_data(args.test_data)
    
    # åŸ·è¡Œè©•ä¼°
    results = evaluator.evaluate(
        test_data=test_data,
        k_values=args.k_values,
        top_k=args.top_k
    )
    
    # æ‰“å°çµæœ
    evaluator.print_results(results)
    
    # ä¿å­˜çµæœ
    evaluator.save_results(results, args.output)
    
    print("\nâœ… è©•ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
