#!/usr/bin/env python3
"""
è³‡æ–™é›†é©—è­‰è…³æœ¬
ç”¨æ–¼æª¢æŸ¥è³‡æ–™é›†æ˜¯å¦æ­£ç¢ºæ”¾ç½®å’Œå¯è®€å–
"""

import json
import gzip
from pathlib import Path
from typing import Optional

def check_file(filepath: str) -> Optional[dict]:
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”å¯è®€å–"""
    path = Path(filepath)
    
    if not path.exists():
        return None
    
    size_bytes = path.stat().st_size
    size_mb = size_bytes / (1024 * 1024)
    size_gb = size_bytes / (1024 * 1024 * 1024)
    
    # å˜—è©¦è®€å–ç¬¬ä¸€è¡Œä»¥é©—è­‰æ ¼å¼
    is_gzip = filepath.endswith('.gz')
    open_func = gzip.open if is_gzip else open
    mode = 'rt' if is_gzip else 'r'
    
    try:
        with open_func(filepath, mode, encoding='utf-8') as f:
            first_line = f.readline()
            data = json.loads(first_line.strip())
            
            # è¨ˆç®—è¡Œæ•¸ï¼ˆåƒ…å‰ 1000 è¡Œä»¥åŠ å¿«é€Ÿåº¦ï¼‰
            count = 1
            for i, _ in enumerate(f):
                count += 1
                if i >= 999:  # å·²è®€ 1 è¡Œ + 999 è¡Œ = 1000 è¡Œ
                    break
            
            return {
                'exists': True,
                'size_mb': size_mb,
                'size_gb': size_gb,
                'readable': True,
                'format': 'gzip' if is_gzip else 'json',
                'sample_keys': list(data.keys()),
                'estimated_lines': count if count < 1000 else f"{count}+"
            }
    except Exception as e:
        return {
            'exists': True,
            'size_mb': size_mb,
            'size_gb': size_gb,
            'readable': False,
            'error': str(e)
        }

def main():
    print("=" * 70)
    print("ğŸ“Š è³‡æ–™é›†é©—è­‰å·¥å…·")
    print("=" * 70)
    
    # å®šç¾©è¦æª¢æŸ¥çš„è³‡æ–™é›†
    datasets = {
        "California POI (å£“ç¸®)": "datasets/meta-California.json.gz",
        "California è©•è«– (å£“ç¸®)": "datasets/review-California.json.gz",
        "æ¸¬è©¦ POI (æœªå£“ç¸®)": "datasets/meta-other.json",
        "æ¸¬è©¦ POI (å£“ç¸®)": "datasets/meta-other.json.gz",
        "æ¸¬è©¦è©•è«– (æœªå£“ç¸®)": "datasets/review-other.json",
        "æ¸¬è©¦è©•è«– (å£“ç¸®)": "datasets/review-other.json.gz",
    }
    
    found_datasets = []
    missing_datasets = []
    
    for name, filepath in datasets.items():
        print(f"\næª¢æŸ¥: {name}")
        print(f"è·¯å¾‘: {filepath}")
        
        result = check_file(filepath)
        
        if result is None:
            print(f"  âŒ æª”æ¡ˆä¸å­˜åœ¨")
            missing_datasets.append((name, filepath))
        elif not result['readable']:
            print(f"  âš ï¸  æª”æ¡ˆå­˜åœ¨ä½†ç„¡æ³•è®€å–")
            print(f"  å¤§å°: {result['size_mb']:.2f} MB")
            print(f"  éŒ¯èª¤: {result.get('error', 'Unknown')}")
        else:
            print(f"  âœ… æª”æ¡ˆæ­£å¸¸")
            if result['size_gb'] >= 1:
                print(f"  å¤§å°: {result['size_gb']:.2f} GB")
            else:
                print(f"  å¤§å°: {result['size_mb']:.2f} MB")
            print(f"  æ ¼å¼: {result['format']}")
            print(f"  ä¼°è¨ˆè¡Œæ•¸: {result['estimated_lines']}")
            print(f"  è³‡æ–™æ¬„ä½: {', '.join(result['sample_keys'][:10])}")
            found_datasets.append((name, filepath, result))
    
    # ç¸½çµ
    print("\n" + "=" * 70)
    print("ğŸ“‹ æª¢æŸ¥ç¸½çµ")
    print("=" * 70)
    print(f"âœ… æ‰¾åˆ° {len(found_datasets)} å€‹å¯ç”¨è³‡æ–™é›†")
    print(f"âŒ ç¼ºå°‘ {len(missing_datasets)} å€‹è³‡æ–™é›†")
    
    if found_datasets:
        print("\nå¯ç”¨è³‡æ–™é›†ï¼š")
        for name, filepath, result in found_datasets:
            size_str = f"{result['size_gb']:.2f} GB" if result['size_gb'] >= 1 else f"{result['size_mb']:.2f} MB"
            print(f"  â€¢ {name}: {filepath} ({size_str})")
    
    if missing_datasets:
        print("\nç¼ºå°‘çš„è³‡æ–™é›†ï¼š")
        for name, filepath in missing_datasets:
            print(f"  â€¢ {name}: {filepath}")
    
    # æ¨è–¦ä½¿ç”¨çš„è³‡æ–™é›†
    print("\n" + "=" * 70)
    print("ğŸ’¡ ä½¿ç”¨å»ºè­°")
    print("=" * 70)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ California è³‡æ–™é›†
    has_california = any('California' in name for name, _, _ in found_datasets)
    has_other = any('other' in filepath for _, filepath, _ in found_datasets)
    
    if has_california:
        print("ğŸ¯ æ¨è–¦ä½¿ç”¨å®Œæ•´ California è³‡æ–™é›†é€²è¡Œè¨“ç·´ï¼š")
        print("   python train_model.py \\")
        print("     --meta-path datasets/meta-California.json.gz \\")
        print("     --review-path datasets/review-California.json.gz \\")
        print("     --max-pois 50000 \\")
        print("     --max-reviews 500000 \\")
        print("     --epochs 20")
    
    if has_other:
        print("\nğŸ§ª å¿«é€Ÿæ¸¬è©¦ä½¿ç”¨å°å‹è³‡æ–™é›†ï¼š")
        print("   python train_model.py \\")
        print("     --meta-path datasets/meta-other.json \\")
        print("     --review-path datasets/review-other.json \\")
        print("     --max-pois 1000 \\")
        print("     --max-reviews 5000 \\")
        print("     --epochs 5")
    
    if not has_california and not has_other:
        print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„è³‡æ–™é›†ï¼")
        print("è«‹ç¢ºèªè³‡æ–™é›†æª”æ¡ˆå·²æ”¾ç½®åœ¨ datasets/ ç›®éŒ„ä¸­ã€‚")
    
    print("\n" + "=" * 70)

if __name__ == "__main__":
    main()
