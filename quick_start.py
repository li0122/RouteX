"""
å¿«é€Ÿé–‹å§‹æŒ‡å— - RouteX æ—…éŠæ¨è–¦ç³»çµ±
æœ€ç°¡å–®çš„ä½¿ç”¨æ–¹å¼
"""

import torch

try:
    from simple_llm_filter import SimpleLLMFilter
    LLM_FILTER_AVAILABLE = True
except ImportError:
    LLM_FILTER_AVAILABLE = False


def quick_start_without_llm():
    """å¿«é€Ÿé–‹å§‹ - ä¸ä½¿ç”¨LLMéæ¿¾"""
    
    print("ğŸš€ å¿«é€Ÿé–‹å§‹ - åŸºæœ¬æ¨è–¦")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–æ¨è–¦å™¨
    from route_aware_recommender import create_route_recommender, OSRMClient
    
    # å‰µå»ºOSRMå®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨å…¬é–‹OSRMæœå‹™å™¨ï¼‰
    osrm_client = OSRMClient(server_url="http://router.project-osrm.org")
    
    recommender = create_route_recommender(
        poi_data_path='datasets/meta-California.json.gz',
        model_checkpoint='models/travel_dlrm.pth',
        device='cuda' if __import__('torch').cuda.is_available() else 'cpu',
        enable_spatial_index=True,
        enable_async=True
    )
    
    # è¨­ç½®OSRMå®¢æˆ¶ç«¯
    recommender.osrm_client = osrm_client
    
    # 2. ç²å–æ¨è–¦
    recommendations = recommender.recommend(
        user_id=1,
        current_lat=37.7749,      # èˆŠé‡‘å±±
        current_lon=-122.4194,
        destination_lat=37.8199,   # é‡‘é–€å¤§æ©‹
        destination_lon=-122.4783,
        top_k=5
    )
    
    # 3. é¡¯ç¤ºçµæœ
    print(f"\nâœ… ç²å¾— {len(recommendations)} å€‹æ¨è–¦:")
    for i, rec in enumerate(recommendations, 1):
        poi = rec['poi']
        print(f"{i}. {poi['name']} - {rec['score']:.3f}åˆ†")
    
    return recommendations


def quick_start_with_llm():
    """å¿«é€Ÿé–‹å§‹ - ä½¿ç”¨LLMéæ¿¾ï¼ˆæ¨è–¦ï¼‰"""
    
    print("\n\nğŸ¤– å¿«é€Ÿé–‹å§‹ - LLMæ™ºèƒ½æ¨è–¦")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–æ¨è–¦å™¨ï¼ˆå•Ÿç”¨LLMï¼‰
    from route_aware_recommender import create_route_recommender, OSRMClient
    
    # å‰µå»ºOSRMå®¢æˆ¶ç«¯
    osrm_client = OSRMClient(server_url="http://router.project-osrm.org")
    
    recommender = create_route_recommender(
        poi_data_path='datasets/meta-California.json.gz',
        model_checkpoint='models/travel_dlrm.pth',
        device='cuda' if __import__('torch').cuda.is_available() else 'cpu',
        enable_spatial_index=True,
        enable_async=True
    )
    
    # è¨­ç½®OSRMå®¢æˆ¶ç«¯
    recommender.osrm_client = osrm_client
    
    # ğŸ”‘ å•Ÿç”¨LLMéæ¿¾
    recommender.enable_llm_filter = True
    if LLM_FILTER_AVAILABLE:
        try:
            from simple_llm_filter import SimpleLLMFilter
            recommender.llm_filter = SimpleLLMFilter()
            print("âœ… LLMéæ¿¾å™¨å·²å•Ÿç”¨")
        except Exception as e:
            print(f"âš ï¸ LLMéæ¿¾å™¨å•Ÿç”¨å¤±æ•—: {e}")
            recommender.enable_llm_filter = False
    else:
        print("âš ï¸ LLMéæ¿¾å™¨ä¸å¯ç”¨")
        recommender.enable_llm_filter = False
    
    # 2. ç²å–æ¨è–¦ï¼ˆè‡ªå‹•ä½¿ç”¨LLMéæ¿¾ï¼‰
    recommendations = recommender.recommend(
        user_id=1,
        current_lat=37.7749,
        current_lon=-122.4194,
        destination_lat=37.8199,
        destination_lon=-122.4783,
        top_k=3
    )
    
    # 3. é¡¯ç¤ºçµæœ
    print(f"\nâœ… LLMéæ¿¾å¾Œçš„æ¨è–¦:")
    for i, rec in enumerate(recommendations, 1):
        poi = rec['poi']
        llm_status = "âœ… LLMå¯©æ ¸é€šé" if rec.get('llm_approved') else ""
        print(f"{i}. {poi['name']} - {rec['score']:.3f}åˆ† {llm_status}")
    
    return recommendations


if __name__ == "__main__":
    print("ğŸ® RouteX å¿«é€Ÿé–‹å§‹æŒ‡å—\n")
    
    # æ–¹å¼1: åŸºæœ¬æ¨è–¦ï¼ˆå¿«é€Ÿï¼‰
    quick_start_without_llm()
    
    # æ–¹å¼2: LLMæ™ºèƒ½æ¨è–¦ï¼ˆç²¾æº–ä½†è¼ƒæ…¢ï¼‰
    print("\nâ¸ï¸  æŒ‰ Enter æ¸¬è©¦LLMæ™ºèƒ½æ¨è–¦...")
    input()
    quick_start_with_llm()
    
    print("\n\nğŸ‰ å®Œæˆ! æŸ¥çœ‹ demo_complete_usage.py äº†è§£æ›´å¤šåŠŸèƒ½")
